01:05:06.250 TKD [main] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
01:05:08.707 TKD [main] WARN  o.apache.spark.metrics.MetricsSystem - Using default name DAGScheduler for source because spark.app.id is not set.
01:05:14.657 TKD [dag-scheduler-event-loop] ERROR org.apache.hadoop.util.Shell - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278) [hadoop-common-2.2.0.jar:na]
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300) [hadoop-common-2.2.0.jar:na]
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293) [hadoop-common-2.2.0.jar:na]
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:76) [hadoop-common-2.2.0.jar:na]
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:362) [hadoop-mapreduce-client-core-2.2.0.jar:na]
	at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$32.apply(SparkContext.scala:1016) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$32.apply(SparkContext.scala:1016) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.map(Option.scala:146) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:176) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.getPartitions(MapPartitionsWithPreparationRDD.scala:40) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:82) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.sql.execution.ShuffledRowRDD.getDependencies(ShuffledRowRDD.scala:59) [spark-sql_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:226) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:224) [spark-core_2.11-1.5.2.jar:1.5.2]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.7.jar:na]
	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:224) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.scheduler.DAGScheduler.visit$1(DAGScheduler.scala:351) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.scheduler.DAGScheduler.getParentStages(DAGScheduler.scala:363) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.scheduler.DAGScheduler.getParentStagesAndId(DAGScheduler.scala:266) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.scheduler.DAGScheduler.newResultStage(DAGScheduler.scala:300) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:734) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1466) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447) [spark-core_2.11-1.5.2.jar:1.5.2]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) [spark-core_2.11-1.5.2.jar:1.5.2]
01:05:15.326 TKD [dag-scheduler-event-loop] WARN   - Your hostname, Lubov resolves to a loopback/non-reachable address: fe80:0:0:0:417a:422f:f2d4:d8bd%10, but we couldn't find any external IP address!
